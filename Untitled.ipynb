{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           (None, 94208)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 94208)        0           input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_img (Dense)               (None, 512)          48234496    dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           (None, 90)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_img (BatchN (None, 512)          2048        dense_img[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "emb_text (Embedding)            (None, 90, 512)      14848       input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 1, 512)       0           batch_normalization_img[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "input_47 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_48 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     multiple             2099200     lambda_12[0][0]                  \n",
      "                                                                 input_47[0][0]                   \n",
      "                                                                 input_48[0][0]                   \n",
      "                                                                 dropout_24[0][0]                 \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 90, 512)      0           emb_text[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_softmax (TimeD (None, 90, 29)       14877       lstm[1][0]                       \n",
      "==================================================================================================\n",
      "Total params: 50,365,469\n",
      "Trainable params: 50,364,445\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "File to train the NIC model, based on the paper:\n",
    "\n",
    "https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf\n",
    "'''\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from NIC import model\n",
    "from preprocessing.text import create_tokenizer\n",
    "from utils import batch_generator\n",
    "\n",
    "from TensorBoardCaption import TensorBoardCaption\n",
    "\n",
    "\n",
    "def training(dirs_dict, lr, decay, reg, batch_size, epochs, max_len, initial_epoch, previous_model = None):\n",
    "\n",
    "    dict_dir = dirs_dict['dict_dir']\n",
    "    token_dir = dirs_dict['token_dir']\n",
    "    train_dir = dirs_dict['train_dir']\n",
    "    dev_dir = dirs_dict['dev_dir']\n",
    "    params_dir = dirs_dict['params_dir']\n",
    "\n",
    "    # Use Tokenizer to create vocabulary\n",
    "    tokenizer = create_tokenizer(train_dir, token_dir, start_end = True)\n",
    "    \n",
    "    # Progressive loading\n",
    "    # if batch size of training set is 30 and total 30000 sentences, then 1000 steps.\n",
    "    # if batch size of dev set is 50 and total 5000 sentences, then 100 steps.\n",
    "    generator_train = batch_generator(batch_size, max_len, tokenizer, dict_dir, train_dir, token_dir)\n",
    "    generator_dev = batch_generator(50, max_len, tokenizer, dict_dir, dev_dir, token_dir)\n",
    "\n",
    "    vocab_size = tokenizer.num_words or (len(tokenizer.word_index)+1)\n",
    "\n",
    "    # Define NIC model structure\n",
    "    NIC_model = model(vocab_size, max_len, reg)\n",
    "\n",
    "    if not previous_model:\n",
    "        NIC_model.summary()\n",
    "        plot_model(NIC_model, to_file='./model.png',show_shapes=True)\n",
    "    else:\n",
    "        NIC_model.load_weights(previous_model, by_name = True, skip_mismatch=True)\n",
    "\n",
    "    # Define checkpoint callback\n",
    "    file_path = params_dir + '/model-ep{epoch:03d}-loss{loss:.4f}-val_loss{val_loss:.4f}.h5'\n",
    "    '''checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_weights_only = True, period=1)\n",
    "    tbc = TensorBoardCaption(tokenizer, vocab_size, max_len, log_dir = './logs', \n",
    "                            feed_pics_dir = './put-your-image-here',\n",
    "                            model_params_dir = params_dir)'''\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    NIC_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr = lr, decay=decay), metrics=['accuracy'])\n",
    "\n",
    "    # train\n",
    "    print(\"hello world\")\n",
    "    #NIC_model.fit_generator(generator_train, steps_per_epoch=30000//batch_size, epochs=epochs,\n",
    "                            #callbacks=[checkpoint, tbc],\n",
    "                            #validation_data = generator_dev, validation_steps = 100, initial_epoch = initial_epoch)\n",
    "\n",
    "    return generator_train\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dict_dir = './datasets/features_dict.pkl'\n",
    "    train_dir = './datasets/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "    dev_dir = './datasets/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "    token_dir = './datasets/Flickr8k_text/Flickr8k.token.txt'\n",
    "    # where to put the model weigths\n",
    "    params_dir = './model-params'\n",
    "\n",
    "    dirs_dict={'dict_dir':dict_dir, 'train_dir':train_dir, 'dev_dir':dev_dir, 'token_dir':token_dir, 'params_dir':params_dir}\n",
    "    \n",
    "    generator_train = training(dirs_dict, lr=0.001, decay=0., reg = 1e-4, batch_size = 100, epochs = 2, max_len = 90, initial_epoch = 0, previous_model = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    next(generator_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "(img_features,raw_sentences) = next(generator_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 94208)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 18, 15, ...,  0,  0,  0],\n",
       "       [17, 22, 12, ...,  0,  0,  0],\n",
       "       [17, 22, 12, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [17, 18, 16, ...,  0,  0,  0],\n",
       "       [17, 18, 15, ...,  0,  0,  0],\n",
       "       [17, 22, 12, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 512)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.        , 1.1222678 , 3.4907746 , ..., 0.58888793, 5.064062  ,\n",
       "         1.8853363 ],\n",
       "        [0.        , 1.1222678 , 3.4907746 , ..., 0.58888793, 5.064062  ,\n",
       "         1.8853363 ],\n",
       "        [0.        , 1.1222678 , 3.4907746 , ..., 0.58888793, 5.064062  ,\n",
       "         1.8853363 ],\n",
       "        ...,\n",
       "        [0.        , 1.6412555 , 2.640316  , ..., 0.58888793, 5.064062  ,\n",
       "         1.8853363 ],\n",
       "        [0.        , 1.6412555 , 2.640316  , ..., 0.58888793, 5.064062  ,\n",
       "         1.8853363 ],\n",
       "        [0.        , 1.6412555 , 2.640316  , ..., 0.58888793, 5.064062  ,\n",
       "         1.8853363 ]], dtype=float32), array([[17, 22, 12, ...,  3, 10,  3],\n",
       "        [17, 22, 12, ...,  0,  0,  0],\n",
       "        [17, 18, 16, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [17, 18, 16, ...,  0,  0,  0],\n",
       "        [17, 18, 16, ...,  0,  0,  0],\n",
       "        [17, 22, 12, ...,  0,  0,  0]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 90, 29)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-ee70c16e787c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerator_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not callable"
     ]
    }
   ],
   "source": [
    "generator_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dir = './datasets/features_dict.pkl'\n",
    "train_dir = './datasets/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "dev_dir = './datasets/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "token_dir = './datasets/Flickr8k_text/Flickr8k.token.txt'\n",
    "# where to put the model weigths\n",
    "params_dir = './model-params'\n",
    "\n",
    "dirs_dict={'dict_dir':dict_dir, 'train_dir':train_dir, 'dev_dir':dev_dir, 'token_dir':token_dir, 'params_dir':params_dir}\n",
    "\n",
    "\n",
    "dict_dir = dirs_dict['dict_dir']\n",
    "token_dir = dirs_dict['token_dir']\n",
    "train_dir = dirs_dict['train_dir']\n",
    "dev_dir = dirs_dict['dev_dir']\n",
    "params_dir = dirs_dict['params_dir']\n",
    "\n",
    "# Use Tokenizer to create vocabulary\n",
    "tokenizer = create_tokenizer(train_dir, token_dir, start_end = True)\n",
    "\n",
    "# Progressive loading\n",
    "# if batch size of training set is 30 and total 30000 sentences, then 1000 steps.\n",
    "# if batch size of dev set is 50 and total 5000 sentences, then 100 steps.\n",
    "generator_train = batch_generator(100, 90, tokenizer, dict_dir, train_dir, token_dir)\n",
    "#generator_dev = batch_generator(50, max_len, tokenizer, dict_dir, dev_dir, token_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-6a6ffde383d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not callable"
     ]
    }
   ],
   "source": [
    "batch_generator(100, 90, tokenizer, dict_dir, train_dir, token_dir)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
